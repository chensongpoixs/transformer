{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tools.data_loader import MTDataset\n",
    "from model.tf_model import make_model\n",
    "import logging\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "from beam_decoder import beam_search\n",
    "from model.train_utils import  MultiGPULossCompute, get_std_opt\n",
    "from tools.tokenizer_utils import chinese_tokenizer_load\n",
    "from tools.create_exp_folder import create_exp_folder\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s-%(name)s-%(levelname)s-%(message)s', level=logging.INFO)\n",
    "\n",
    "def run_epoch(data, model, loss_compute):\n",
    "    total_tokens = 0.   # 初始化token的总数\n",
    "    total_loss = 0.  # 初始化总损失\n",
    "\n",
    "    # 遍历整个数据集（数据为batch的形式）\n",
    "    for batch in tqdm(data):  # tqdm用于显示处理进度条\n",
    "        # 模型前向传播，得到预测结果out\n",
    "        # batch.src：输入的源语言数据，batch.trg：目标语言数据，batch.src_mask：源语言mask，batch.trg_mask：目标语言mask\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "\n",
    "        # 使用loss_compute计算损失\n",
    "        # batch.trg_y：目标输出数据，batch.ntokens：非填充部分的token数量（有效token数量）\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "\n",
    "        # 累加损失和有效tokens的数量\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "\n",
    "    # 返回每个token的平均损失\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "def train(train_data, dev_data, model, model_par, criterion, optimizer):\n",
    "    \"\"\"训练并保存模型\"\"\"\n",
    "    # best_bleu_score初始化\n",
    "    best_bleu_score = -float('inf')  # 初始最佳BLEU分数为负无穷\n",
    "    # 创建保存权重的路径\n",
    "    exp_folder, weights_folder = create_exp_folder()\n",
    "\n",
    "    # 开始训练循环，迭代每个epoch\n",
    "    for epoch in range(1, config.epoch_num + 1):\n",
    "        logging.info(f\"第{epoch}轮模型训练与验证\")\n",
    "        # 设置模型为训练模式\n",
    "        model.train()\n",
    "        # 进行一个epoch的训练，返回当前的训练损失\n",
    "        train_loss = run_epoch(train_data, model_par,\n",
    "                               MultiGPULossCompute(model.generator, criterion, config.device_id, optimizer))\n",
    "\n",
    "        # 设置模型为评估模式（即不计算梯度，优化）\n",
    "        model.eval()\n",
    "        # 进行一个epoch的验证，返回当前的验证损失\n",
    "        dev_loss = run_epoch(dev_data, model_par,\n",
    "                             MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n",
    "\n",
    "        # 计算模型在验证集（dev_data）上的BLEU分数\n",
    "        bleu_score = evaluate(dev_data, model)\n",
    "        logging.info(f\"Epoch: {epoch}, train_loss: {train_loss:.3f}, val_loss: {dev_loss:.3f}, Bleu Score: {bleu_score:.2f}\\n\")\n",
    "\n",
    "        # 如果当前epoch的模型的BLEU分数更优，则保存最佳模型\n",
    "        if bleu_score > best_bleu_score:\n",
    "            # 如果之前已存在最优模型，先删除\n",
    "            if best_bleu_score != -float('inf'):\n",
    "                old_model_path = f\"{weights_folder}/best_bleu_{best_bleu_score:.2f}.pth\"\n",
    "                if os.path.exists(old_model_path):\n",
    "                    os.remove(old_model_path)\n",
    "\n",
    "            model_path_best = f\"{weights_folder}/best_bleu_{bleu_score:.2f}.pth\"\n",
    "            # 保存当前模型的状态字典到指定路径\n",
    "            torch.save(model.state_dict(), model_path_best)\n",
    "            # 更新最佳BLEU分数\n",
    "            best_bleu_score = bleu_score\n",
    "            # 记录最佳模型保存信息到日志\n",
    "\n",
    "        # 保存当前模型（最后一次训练）\n",
    "        if epoch == config.epoch_num:  # 判断是否达到设定的训练轮数\n",
    "            model_path_last = f\"{weights_folder}/last_bleu_{bleu_score:.2f}.pth\"  # 构建模型保存路径，包含BLEU分数\n",
    "            torch.save(model.state_dict(), model_path_last)  # 保存模型的状态字典\n",
    "\n",
    "def evaluate(data, model):\n",
    "    \"\"\"在data上用训练好的模型进行预测，打印模型翻译结果\"\"\"\n",
    "    sp_chn = chinese_tokenizer_load()  # 加载中文分词器\n",
    "    trg = []  # 存储目标句子（真实句子）\n",
    "    res = []  # 存储模型翻译的结果\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存和计算\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        for batch in tqdm(data):  # 使用tqdm显示进度条\n",
    "            cn_sent = batch.trg_text  # 获取当前批次的中文句子\n",
    "            src = batch.src   # 获取当前批次的源语言（英文）句子\n",
    "            src_mask = (src != 0).unsqueeze(-2)    # 为源语言句子创建mask，排除padding部分\n",
    "\n",
    "            # 使用束搜索生成模型翻译结果\n",
    "            decode_result, _ = beam_search(model, src, src_mask, config.max_len,\n",
    "                                               config.padding_idx, config.bos_idx, config.eos_idx,\n",
    "                                               config.beam_size, config.device)\n",
    "\n",
    "            # `decode_result`是一个包含多个翻译结果的列表，取最优结果\n",
    "            decode_result = [h[0] for h in decode_result]\n",
    "            # 解码后的id转为中文句子\n",
    "            translation = [sp_chn.decode_ids(_s) for _s in decode_result]\n",
    "            trg.extend(cn_sent)  # 将当前批次的真实句子添加到`trg`中\n",
    "            res.extend(translation)  # 将模型的翻译结果添加到`res`中\n",
    "\n",
    "    # 计算BLEU分数，使用SacreBLEU工具库\n",
    "    trg = [trg]  # 真实目标句子\n",
    "    bleu = sacrebleu.corpus_bleu(res, trg, tokenize='zh')  # 计算BLEU分数\n",
    "    return float(bleu.score)  # 返回BLEU分数\n",
    "\n",
    "\n",
    "def test(data, model, criterion):\n",
    "    with torch.no_grad():\n",
    "        # 加载模型\n",
    "        model.load_state_dict(torch.load(config.model_path))\n",
    "        model_par = torch.nn.DataParallel(model)\n",
    "        model.eval()\n",
    "        # 开始预测\n",
    "        test_loss = run_epoch(data, model_par,\n",
    "                              MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n",
    "        bleu_score = evaluate(data, model, 'test')\n",
    "        logging.info('Test loss: {},  Bleu Score: {}'.format(test_loss, bleu_score))\n",
    "\n",
    "\n",
    "def run():\n",
    "    # 创建训练数据集和开发数据集\n",
    "    # 使用MTDataset类分别加载训练数据和开发数据\n",
    "    train_dataset = MTDataset(config.train_data_path)   # 初始化训练数据集，使用配置中指定的训练数据路径\n",
    "    dev_dataset = MTDataset(config.dev_data_path)   # 初始化开发数据集，使用配置中指定的开发数据路径\n",
    "    test_dataset = MTDataset(config.test_data_path)\n",
    "\n",
    "    # 创建训练数据加载器，用于训练过程中批量加载数据\n",
    "    # shuffle=True 表示在每个epoch开始时会打乱数据顺序，以增加模型的泛化能力\n",
    "    # batch_size=config.batch_size 表示每个批次的样本数量，具体值由配置文件决定\n",
    "    # collate_fn=train_dataset.collate_fn 表示自定义的数据整理函数，用于处理每个批次的数据\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size,\n",
    "                                  collate_fn=train_dataset.collate_fn)\n",
    "    print(\"训练数据加载完成: {} 个批次, train_dataloader:{}   \".format(len(train_dataloader), train_dataloader));\n",
    "    dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "                                collate_fn=dev_dataset.collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "                                 collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    # 初始化模型\n",
    "    # 构建Transformer模型，传入词汇表大小、层数、模型维度等参数\n",
    "    # 这些参数均来自config配置文件\n",
    "    # 创建Transformer模型实例\n",
    "    model = make_model(config.src_vocab_size, # src_vocab_size:源语言词汇表大小 32000    \n",
    "                       config.tgt_vocab_size, # tgt_vocab_size:目标语言词汇表大小 32000\n",
    "                       config.n_layers, # n_layers:Transformer模型的层数 6\n",
    "                       config.d_model, # d_model:模型的维度 512\n",
    "                       config.d_ff,     # d_ff:前馈神经网络的维度 2048\n",
    "                       config.n_heads,  # n_heads:多头注意力机制的头数 8\n",
    "                       config.dropout); # dropout:dropout率 0.1\n",
    "\n",
    "    #  将模型包装成数据并行模式,这样可以在多个GPU上并行处理数据，提高训练效率\n",
    "    model_par = torch.nn.DataParallel(model);\n",
    "\n",
    "    # 训练阶段，选择损失函数和优化器\n",
    "    # CrossEntropyLoss是常见的分类问题损失函数，ignore_index=0表示忽略填充部分\n",
    "    # reduction='sum'表示计算损失时会对所有token的损失求和\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum');\n",
    "\n",
    "    # 调用get_std_opt函数获取标准的Noam优化器，这通常包括学习率调度器（如预热后衰减）\n",
    "    optimizer = get_std_opt(model);\n",
    "\n",
    "    # 开始训练\n",
    "    train(train_dataloader, dev_dataloader, model, model_par, criterion, optimizer);\n",
    "    # test(test_dataloader, model, criterion)\n",
    "\n",
    "# 256路  32 核  96GB 内存  4TB SSD\n",
    "# 8卡  A100 40GB\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
